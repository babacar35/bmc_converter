import { ProjectContext, BMCData, AIAnalysis } from '@/types/bmc';
import { LLMProvider_Interface, LLMConfig, LLMResponse } from '../types';
import { buildAnalysisPrompt, SYSTEM_PROMPTS } from '../prompts';

interface OllamaResponse {
  response: string;
  model: string;
  created_at: string;
  done: boolean;
  context?: number[];
  total_duration?: number;
  load_duration?: number;
  prompt_eval_count?: number;
  prompt_eval_duration?: number;
  eval_count?: number;
  eval_duration?: number;
}

interface OllamaChatResponse {
  message: {
    role: string;
    content: string;
  };
  model: string;
  created_at: string;
  done: boolean;
  total_duration?: number;
  load_duration?: number;
  prompt_eval_count?: number;
  prompt_eval_duration?: number;
  eval_count?: number;
  eval_duration?: number;
}

export class OllamaProvider implements LLMProvider_Interface {
  name: 'ollama' = 'ollama';
  config: LLMConfig;
  private baseUrl: string;

  constructor(config: LLMConfig) {
    this.config = config;
    this.baseUrl = config.baseUrl || process.env.NEXT_PUBLIC_OLLAMA_BASE_URL || 'http://localhost:11434';
    
    console.log('Ollama provider initialized with base URL:', this.baseUrl);
    console.log('Ollama model:', config.model);
  }

  async testConnection(): Promise<boolean> {
    try {
      console.log('Testing Ollama connection...');
      
      // Test si Ollama est accessible
      const response = await fetch(`${this.baseUrl}/api/tags`, {
        method: 'GET',
        headers: { 'Content-Type': 'application/json' }
      });

      if (!response.ok) {
        throw new Error(`Ollama server not accessible: ${response.status}`);
      }

      const data = await response.json();
      console.log('Available Ollama models:', data.models?.map((m: any) => m.name) || []);

      // Test de g√©n√©ration simple
      await this.generateResponse(SYSTEM_PROMPTS.CONNECTION_TEST);
      console.log('Ollama connection successful');
      return true;
      
    } catch (error) {
      console.error('Ollama connection failed:', error);
      return false;
    }
  }

  async generateResponse(
    prompt: string,
    options?: {
      temperature?: number;
      maxTokens?: number;
      systemPrompt?: string;
    }
  ): Promise<LLMResponse> {
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), 60000); // 60s timeout pour Ollama

    try {
      const model = this.config.model || 'deepseek-r1:8b';
      
      // Utiliser l'API chat si un system prompt est fourni
      if (options?.systemPrompt) {
        const url = `${this.baseUrl}/api/chat`;
        const requestBody = {
          model: model,
          messages: [
            {
              role: 'system',
              content: options.systemPrompt
            },
            {
              role: 'user', 
              content: prompt
            }
          ],
          stream: false,
          options: {
            temperature: options?.temperature ?? this.config.temperature ?? 0.1,
            num_predict: options?.maxTokens ?? this.config.maxTokens ?? 1000,
            top_p: 0.8,
            top_k: 10
          }
        };

        const response = await fetch(url, {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify(requestBody),
          signal: controller.signal,
        });

        if (!response.ok) {
          const errorText = await response.text();
          throw new Error(`Ollama API error (${response.status}): ${errorText}`);
        }

        const data: OllamaChatResponse = await response.json();
        
        return {
          content: data.message.content,
          usage: data.prompt_eval_count && data.eval_count ? {
            promptTokens: data.prompt_eval_count,
            completionTokens: data.eval_count,
            totalTokens: data.prompt_eval_count + data.eval_count
          } : undefined,
          model: data.model,
          provider: 'ollama'
        };

      } else {
        // Utiliser l'API generate standard
        const url = `${this.baseUrl}/api/generate`;
        const requestBody = {
          model: model,
          prompt: prompt,
          stream: false,
          options: {
            temperature: options?.temperature ?? this.config.temperature ?? 0.1,
            num_predict: options?.maxTokens ?? this.config.maxTokens ?? 1000,
            top_p: 0.8,
            top_k: 10
          }
        };

        const response = await fetch(url, {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify(requestBody),
          signal: controller.signal,
        });

        if (!response.ok) {
          const errorText = await response.text();
          throw new Error(`Ollama API error (${response.status}): ${errorText}`);
        }

        const data: OllamaResponse = await response.json();
        
        return {
          content: data.response,
          usage: data.prompt_eval_count && data.eval_count ? {
            promptTokens: data.prompt_eval_count,
            completionTokens: data.eval_count,
            totalTokens: data.prompt_eval_count + data.eval_count
          } : undefined,
          model: data.model,
          provider: 'ollama'
        };
      }

    } catch (error) {
      clearTimeout(timeoutId);
      if (error instanceof Error && error.name === 'AbortError') {
        throw new Error('Request timeout - L\'analyse Ollama a pris trop de temps');
      }
      throw error;
    } finally {
      clearTimeout(timeoutId);
    }
  }

  async analyzeSection(
    sectionId: keyof BMCData,
    content: string,
    context: ProjectContext,
    otherSections: Partial<BMCData>
  ): Promise<AIAnalysis> {
    try {
      const prompt = buildAnalysisPrompt(sectionId, content, context, otherSections);
      
      // Utiliser l'approche avec system prompt pour une meilleure qualit√©
      const response = await this.generateResponse(prompt, {
        systemPrompt: `üá´üá∑ Tu es un expert en Business Model Canvas. R√©ponds OBLIGATOIREMENT en FRAN√áAIS avec un JSON valide:
{
  "errors": ["erreur en fran√ßais 1", "erreur en fran√ßais 2"],
  "suggestions": ["suggestion en fran√ßais 1", "suggestion en fran√ßais 2"],
  "score": 85,
  "examples": ["exemple en fran√ßais 1", "exemple en fran√ßais 2"]
}
INTERDIT: Aucun mot anglais dans ta r√©ponse !`
      });

      // Parser la r√©ponse JSON
      let jsonMatch = response.content.match(/\{[\s\S]*\}/);
      
      if (!jsonMatch) {
        // Fallback: chercher le JSON en d√©but/fin de r√©ponse
        const cleanContent = response.content.trim();
        if (cleanContent.startsWith('{') && cleanContent.includes('}')) {
          jsonMatch = [cleanContent];
        } else {
          throw new Error('No JSON found in Ollama response');
        }
      }

      let analysis;
      try {
        // Nettoyer les caract√®res de contr√¥le et probl√©matiques
        let cleanedJson = jsonMatch[0]
          .replace(/[\x00-\x1F\x7F-\x9F]/g, '') // Supprimer les caract√®res de contr√¥le
          .replace(/\n\s*\n/g, '\n')           // Supprimer les lignes vides multiples
          .replace(/,(\s*[}\]])/g, '$1')       // Supprimer les virgules avant } ou ]
          .trim();
        
        analysis = JSON.parse(cleanedJson);
      } catch (parseError) {
        console.error('JSON parse error:', parseError);
        console.error('Original JSON:', jsonMatch[0]);
        
        // Tentative de r√©cup√©ration avec regex plus agressive
        try {
          const fallbackMatch = jsonMatch[0].match(/"errors":\s*\[(.*?)\]\s*,?\s*"suggestions":\s*\[(.*?)\]\s*,?\s*"score":\s*(\d+)\s*,?\s*"examples":\s*\[(.*?)\]/s);
          if (fallbackMatch) {
            analysis = {
              errors: fallbackMatch[1] ? fallbackMatch[1].split('","').map(s => s.replace(/^"|"$/g, '')) : [],
              suggestions: fallbackMatch[2] ? fallbackMatch[2].split('","').map(s => s.replace(/^"|"$/g, '')) : [],
              score: parseInt(fallbackMatch[3]) || 0,
              examples: fallbackMatch[4] ? fallbackMatch[4].split('","').map(s => s.replace(/^"|"$/g, '')) : []
            };
          } else {
            throw new Error('Could not recover JSON structure');
          }
        } catch (fallbackError) {
          throw new Error('Invalid JSON format from Ollama - unable to recover');
        }
      }
      
      // V√©rification que la r√©ponse est bien en fran√ßais
      const allText = [
        ...(analysis.errors || []),
        ...(analysis.suggestions || []), 
        ...(analysis.examples || [])
      ].join(' ').toLowerCase();
      
      // D√©tection basique de mots anglais courants
      const englishWords = ['the', 'and', 'or', 'but', 'this', 'that', 'with', 'from', 'they', 'have', 'would', 'could', 'should', 'example', 'consider', 'feature', 'section', 'impact', 'business', 'customers'];
      const hasEnglish = englishWords.some(word => allText.includes(` ${word} `) || allText.includes(`${word} `) || allText.includes(` ${word}`));
      
      if (hasEnglish) {
        console.warn('‚ö†Ô∏è Ollama responded in English, requesting French response...');
        return {
          sectionId,
          errors: ['R√©ponse en anglais d√©tect√©e - Merci de reformuler en fran√ßais'],
          suggestions: ['Veuillez relancer l\'analyse pour obtenir une r√©ponse en fran√ßais'],
          score: 0,
          examples: ['L\'IA doit r√©pondre uniquement en fran√ßais pour une meilleure compr√©hension']
        };
      }
      
      return {
        sectionId,
        errors: analysis.errors || [],
        suggestions: analysis.suggestions || [],
        score: analysis.score || 0,
        examples: analysis.examples || []
      };

    } catch (error) {
      console.error('Ollama analysis error:', error);
      
      let errorMessage = 'Erreur d\'analyse Ollama';
      if (error instanceof Error) {
        if (error.message.includes('timeout')) {
          errorMessage = 'Analyse Ollama timeout - Le mod√®le local prend trop de temps';
        } else if (error.message.includes('not accessible')) {
          errorMessage = 'Ollama non accessible - V√©rifiez que le serveur local est d√©marr√©';
        } else if (error.message.includes('No JSON')) {
          errorMessage = 'R√©ponse Ollama malform√©e - Le mod√®le a du mal avec le JSON';
        } else if (error.message.includes('model')) {
          errorMessage = `Mod√®le ${this.config.model} non trouv√© - V√©rifiez qu'il est install√© localement`;
        }
      }
      
      return {
        sectionId,
        errors: [errorMessage],
        suggestions: [
          'V√©rifiez qu\'Ollama est d√©marr√© (ollama serve)',
          `V√©rifiez que le mod√®le ${this.config.model} est install√©`,
          'R√©essayez l\'analyse'
        ],
        score: 0,
        examples: []
      };
    }
  }
}